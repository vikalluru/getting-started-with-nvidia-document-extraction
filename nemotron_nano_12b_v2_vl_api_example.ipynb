{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Query the Nemotron Nano 12B v2 VL API\n",
    "\n",
    "This notebook demonstrates how to interact with the NVIDIA Nemotron Nano 12B v2 VL API using Python. \n",
    "It covers launching the NIM container, sending basic vision requests, handling streaming, using reasoning capabilities, and performing text-only queries.\n",
    "\n",
    "Reference: [NVIDIA Docs](https://docs.nvidia.com/nim/vision-language-models/latest/examples/nemotron-nano-12b-v2-vl/api.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Launch NIM\n",
    "\n",
    "Before running the Python code, you need to launch the Nemotron Nano 12B NIM container. \n",
    "Run the following command in your terminal:\n",
    "\n",
    "```bash\n",
    "# Choose a container name for bookkeeping \n",
    "export CONTAINER_NAME=\"nvidia-nemotron-nano-12b-v2-vl\" \n",
    "# The container name from the previous ngc registry image list command \n",
    "Repository=\"nemotron-nano-12b-v2-vl\"\n",
    "Latest_Tag=\"1.5.0\"\n",
    "\n",
    "# Choose a VLM NIM Image from NGC \n",
    "export IMG_NAME=\"nvcr.io/nim/nvidia/${Repository}:${Latest_Tag}\" \n",
    "\n",
    "# Choose a path on your system to cache the downloaded models \n",
    "export LOCAL_NIM_CACHE=~/.cache/nim \n",
    "mkdir -p \"$LOCAL_NIM_CACHE\" \n",
    "\n",
    "# Start the VLM NIM \n",
    "docker run -it --rm --name=$CONTAINER_NAME \\\n",
    "  --runtime=nvidia \\\n",
    "  --gpus all \\\n",
    "  --shm-size=32GB \\\n",
    "  -e NGC_API_KEY=$NGC_API_KEY \\\n",
    "  -v \"$LOCAL_NIM_CACHE:/opt/nim/.cache\" \\\n",
    "  -u $(id -u) \\\n",
    "  -p 8000:8000 \\\n",
    "  $IMG_NAME\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install openai requests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Basic Vision Query\n",
    "\n",
    "Send a request with an image URL to describe the image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI(base_url=\"http://0.0.0.0:8000/v1\", api_key=\"not-used\")\n",
    "\n",
    "messages = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "            {\n",
    "                \"type\": \"text\",\n",
    "                \"text\": \"What is in this image?\"\n",
    "            },\n",
    "            {\n",
    "                \"type\": \"image_url\",\n",
    "                \"image_url\": {\n",
    "                    \"url\": \"https://upload.wikimedia.org/wikipedia/commons/thumb/d/dd/Gfp-wisconsin-madison-the-nature-boardwalk.jpg/2560px-Gfp-wisconsin-madison-the-nature-boardwalk.jpg\"\n",
    "                }\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    "]\n",
    "\n",
    "chat_response = client.chat.completions.create(\n",
    "    model=\"nvidia/nemotron-nano-12b-v2-vl\",\n",
    "    messages=messages,\n",
    "    max_tokens=1024,\n",
    "    stream=False,\n",
    ")\n",
    "\n",
    "assistant_message = chat_response.choices[0].message\n",
    "print(assistant_message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Streaming Response\n",
    "\n",
    "Handle streaming response for real-time output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stream = client.chat.completions.create(\n",
    "    model=\"nvidia/nemotron-nano-12b-v2-vl\",\n",
    "    messages=messages,\n",
    "    max_tokens=1024,\n",
    "    stream=True,\n",
    ")\n",
    "\n",
    "print(\"Streaming response:\")\n",
    "for chunk in stream:\n",
    "    delta = chunk.choices[0].delta\n",
    "    if delta and delta.content:\n",
    "        print(delta.content, end=\"\", flush=True)\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Reasoning Capability\n",
    "\n",
    "Enable reasoning by adding `/think` to the system prompt (NOTE: Video inputs do not support reasoning)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reasoning_messages = [\n",
    "    { \"role\": \"system\", \"content\": \"/think\" },\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "            {\n",
    "                \"type\": \"text\",\n",
    "                \"text\": \"What is in this image?\"\n",
    "            },\n",
    "            {\n",
    "                \"type\": \"image_url\",\n",
    "                \"image_url\": {\n",
    "                    \"url\": \"https://upload.wikimedia.org/wikipedia/commons/thumb/d/dd/Gfp-wisconsin-madison-the-nature-boardwalk.jpg/2560px-Gfp-wisconsin-madison-the-nature-boardwalk.jpg\"\n",
    "                }\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    "]\n",
    "\n",
    "chat_response = client.chat.completions.create(\n",
    "    model=\"nvidia/nemotron-nano-12b-v2-vl\",\n",
    "    messages=reasoning_messages,\n",
    "    max_tokens=4096,\n",
    "    stream=False,\n",
    ")\n",
    "\n",
    "print(chat_response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Text-only Queries\n",
    "\n",
    "The model can also be used as a standard LLM for text-only tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_messages = [\n",
    "    { \"role\": \"system\", \"content\": \"You are a helpful assistant\" },\n",
    "    { \"role\": \"user\", \"content\": \"Create a detailed itinerary for a week-long adventure trip through Southeast Asia.\" }\n",
    "]\n",
    "\n",
    "chat_response = client.chat.completions.create(\n",
    "    model=\"nvidia/nemotron-nano-12b-v2-vl\",\n",
    "    messages=text_messages,\n",
    "    max_tokens=4096,\n",
    "    stream=False,\n",
    ")\n",
    "\n",
    "print(chat_response.choices[0].message.content)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}